"""
ç¶²åŸŸè®Šå½¢æ”»æ“Šæª¢æ¸¬æ¨¡çµ„
ç”¨æ–¼æª¢æ¸¬æ¨¡ä»¿ç™½åå–®ç¶²åŸŸçš„å¯ç–‘ç¶²å€
"""

import re
from urllib.parse import urlparse

def detect_domain_spoofing(url_or_message, safe_domains):
    """
    æª¢æ¸¬ç¶²åŸŸè®Šå½¢æ”»æ“Š - è­˜åˆ¥æ¨¡ä»¿ç™½åå–®ç¶²åŸŸçš„å¯ç–‘ç¶²å€
    
    Args:
        url_or_message: è¦æª¢æ¸¬çš„URLæˆ–åŒ…å«URLçš„è¨Šæ¯
        safe_domains: ç™½åå–®ç¶²åŸŸå­—å…¸
        
    Returns:
        dict: {
            'is_spoofed': bool,  # æ˜¯å¦ç‚ºè®Šå½¢ç¶²åŸŸ
            'original_domain': str,  # è¢«æ¨¡ä»¿çš„åŸå§‹ç¶²åŸŸ
            'spoofed_domain': str,  # å¯ç–‘çš„è®Šå½¢ç¶²åŸŸ
            'spoofing_type': str,  # è®Šå½¢é¡å‹
            'risk_explanation': str  # é¢¨éšªèªªæ˜
        }
    """
    # æå–URL
    url_pattern = re.compile(r'https?://[^\s]+|www\.[^\s]+|[a-zA-Z0-9][a-zA-Z0-9-]*\.[a-zA-Z]{2,}[^\s]*')
    urls = url_pattern.findall(url_or_message)
    
    if not urls:
        return {'is_spoofed': False}
    
    for url in urls:
        # æ¨™æº–åŒ–URL
        if not url.startswith(('http://', 'https://')):
            if url.startswith('www.'):
                url = 'https://' + url
            else:
                url = 'https://' + url
        
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ www. å‰ç¶´é€²è¡Œæ¯”è¼ƒ
            domain_without_www = domain[4:] if domain.startswith('www.') else domain
            
            # ğŸš¨ æ–°å¢ï¼šå°ˆé–€æª¢æ¸¬æ”¿åºœç¶²åŸŸè®Šå½¢æ”»æ“Š
            gov_spoofing_result = _detect_government_domain_spoofing(domain_without_www)
            if gov_spoofing_result['is_spoofed']:
                return gov_spoofing_result
            
            # å‰µå»ºæ¨™æº–åŒ–çš„å®‰å…¨ç¶²åŸŸåˆ—è¡¨ï¼ˆåŒ…å«wwwå’Œéwwwç‰ˆæœ¬ï¼‰
            normalized_safe_domains = set()
            for safe_domain in safe_domains.keys():
                safe_domain_lower = safe_domain.lower()
                normalized_safe_domains.add(safe_domain_lower)
                
                # æ·»åŠ wwwå’Œéwwwç‰ˆæœ¬
                if safe_domain_lower.startswith('www.'):
                    normalized_safe_domains.add(safe_domain_lower[4:])
                else:
                    normalized_safe_domains.add('www.' + safe_domain_lower)
            
            # æª¢æŸ¥æ˜¯å¦æœ¬èº«å°±æ˜¯ç™½åå–®ç¶²åŸŸï¼ˆåŒ…å«wwwè®Šé«”ï¼‰
            if domain in normalized_safe_domains or domain_without_www in normalized_safe_domains:
                continue  # é€™æ˜¯æ­£å¸¸çš„ç™½åå–®ç¶²åŸŸï¼Œè·³é
            
            # ğŸš¨ æ–°å¢ï¼šå„ªå…ˆæª¢æŸ¥åŸºç¤åŸŸåç›¸ä¼¼åº¦ï¼ˆå¦‚ cht.tw èˆ‡ cht.com.twï¼‰
            domain_parts = domain_without_www.split('.')
            base_domain = domain_parts[0]
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒåŸºç¤åŸŸåçš„ç™½åå–®ç¶²åŸŸ
            similar_domains = []
            legitimate_variant_found = False
            
            for safe_domain in safe_domains.keys():
                safe_domain_lower = safe_domain.lower()
                safe_parts = safe_domain_lower.split('.')
                safe_base = safe_parts[0]
                
                # è™•ç† www å‰ç¶´ï¼šå¦‚æœç¬¬ä¸€éƒ¨åˆ†æ˜¯ wwwï¼Œå–ç¬¬äºŒéƒ¨åˆ†ä½œç‚ºåŸºç¤ç¶²åŸŸ
                if safe_base == 'www' and len(safe_parts) > 1:
                    safe_base = safe_parts[1]
                
                # å¦‚æœåŸºç¤åŸŸåç›¸åŒï¼Œæª¢æŸ¥æ˜¯å¦ç‚ºåˆæ³•çš„è®Šé«”
                if base_domain == safe_base:
                    # æª¢æŸ¥æ˜¯å¦ç‚ºåˆæ³•çš„åŸŸåè®Šé«”ï¼ˆå¦‚ cht.tw æ˜¯ cht.com.tw çš„è®Šé«”ï¼‰
                    if _is_legitimate_domain_variant(domain_without_www, safe_domain_lower):
                        legitimate_variant_found = True
                        continue  # é€™æ˜¯åˆæ³•çš„åŸŸåè®Šé«”ï¼Œè·³é
                    else:
                        # åŸºç¤åŸŸåç›¸åŒä½†ä¸æ˜¯åˆæ³•è®Šé«”ï¼Œè¨˜éŒ„ç‚ºç›¸ä¼¼åŸŸå
                        site_description = safe_domains.get(safe_domain, "çŸ¥åç¶²ç«™")
                        similar_domains.append({
                            'domain': safe_domain,
                            'description': site_description,
                            'type': 'åŸºç¤åŸŸåç›¸åŒ'
                        })
            
            # å¦‚æœæ‰¾åˆ°åˆæ³•è®Šé«”ï¼Œè·³éæª¢æ¸¬
            if legitimate_variant_found:
                continue
            
            # å¦‚æœæ‰¾åˆ°å¤šå€‹ç›¸ä¼¼åŸŸåï¼Œæä¾›å¤šé‡è­¦å‘Š
            if len(similar_domains) > 0:
                if len(similar_domains) == 1:
                    # å–®ä¸€ç›¸ä¼¼åŸŸå
                    similar_domain = similar_domains[0]
                    return {
                        'is_spoofed': True,
                        'original_domain': similar_domain['domain'],
                        'spoofed_domain': domain,
                        'spoofing_type': "åŸºç¤åŸŸåè®Šå½¢æ”»æ“Š",
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} ç–‘ä¼¼æ¨¡ä»¿æ­£ç‰Œçš„ {similar_domain['domain']} ({similar_domain['description']})ã€‚\n\nè©é¨™é›†åœ˜ä½¿ç”¨ç›¸åŒçš„åŸºç¤åŸŸåä½†ä¸åŒçš„å¾Œç¶´ä¾†è£½ä½œå‡ç¶²ç«™ã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
                else:
                    # å¤šé‡ç›¸ä¼¼åŸŸå - é™åˆ¶æœ€å¤šé¡¯ç¤º3å€‹
                    domain_list = []
                    # åªå–å‰3å€‹ç›¸ä¼¼åŸŸå
                    for similar in similar_domains[:3]:
                        domain_list.append(f"{similar['domain']}({similar['description']})")
                    
                    # å¦‚æœé‚„æœ‰æ›´å¤šç›¸ä¼¼åŸŸåï¼Œæ·»åŠ æç¤º
                    if len(similar_domains) > 3:
                        domain_list.append(f"...ç­‰å…±{len(similar_domains)}å€‹ç›¸ä¼¼ç¶²åŸŸ")
                    
                    return {
                        'is_spoofed': True,
                        'original_domain': 'multiple',
                        'spoofed_domain': domain,
                        'spoofing_type': "å¤šé‡åŸºç¤åŸŸåè®Šå½¢æ”»æ“Š",
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} å¯èƒ½æ¨¡ä»¿å¤šå€‹æ­£ç‰Œç¶²åŸŸï¼š\n\n" + "\n".join([f"â€¢ {d}" for d in domain_list]) + f"\n\né€™æ˜¯å€‹å¯ç–‘çš„ç¶²åŸŸï¼Œè©é¨™é›†åœ˜å¸¸ç”¨ç›¸ä¼¼çš„ç¶²åŸŸåç¨±ä¾†æ··æ·†è¦–è½ï¼Œè£½ä½œå‡ç¶²ç«™é¨™å–å€‹äººè³‡æ–™æˆ–ä¿¡ç”¨å¡è³‡è¨Šã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
            
            # å¿«é€Ÿæª¢æ¸¬ï¼šç‰¹åˆ¥æª¢æŸ¥-twå’Œ-taiwanå¾Œç¶´åŸŸåï¼ˆé«˜é¢¨éšªï¼‰
            for safe_domain in safe_domains.keys():
                safe_domain_lower = safe_domain.lower()
                safe_parts = safe_domain_lower.split('.')
                safe_base = safe_parts[0]
                
                # è™•ç† www å‰ç¶´ï¼šå¦‚æœç¬¬ä¸€éƒ¨åˆ†æ˜¯ wwwï¼Œå–ç¬¬äºŒéƒ¨åˆ†ä½œç‚ºåŸºç¤ç¶²åŸŸ
                if safe_base == 'www' and len(safe_parts) > 1:
                    safe_base = safe_parts[1]
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºåŸºç¤åŸŸååŠ ä¸Š-twæˆ–-taiwanï¼ˆç›´æ¥åˆ¤å®šç‚ºé«˜é¢¨éšªï¼‰
                if base_domain == safe_base + '-tw' or base_domain == safe_base + '-taiwan':
                    site_description = safe_domains.get(safe_domain, "çŸ¥åç¶²ç«™")
                    return {
                        'is_spoofed': True,
                        'original_domain': safe_domain,
                        'spoofed_domain': domain,
                        'spoofing_type': "æ’å…¥é¡å¤–å­—å…ƒæ”»æ“Š",
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} ç–‘ä¼¼æ¨¡ä»¿æ­£ç‰Œçš„ {safe_domain} ({site_description})ã€‚\n\nè©é¨™é›†åœ˜å¸¸ä½¿ç”¨æ·»åŠ '-tw'æˆ–'-taiwan'å­—æ¨£çš„æ‰‹æ³•è£½ä½œå‡ç¶²ç«™ä¾†é¨™å–å€‹äººè³‡æ–™æˆ–ä¿¡ç”¨å¡è³‡è¨Šã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
                
                # æ–°å¢ï¼šæª¢æŸ¥æ˜é¡¯çš„ç¶²åŸŸåç¨±è®Šå½¢æ”»æ“Šï¼ˆå¦‚ fetc-nete æ¨¡ä»¿ fetcï¼‰
                # æª¢æŸ¥æ˜¯å¦ç‚ºåŸºç¤åŸŸåçš„è®Šå½¢ï¼ˆæ’å…¥å­—å…ƒã€æ›¿æ›å­—å…ƒç­‰ï¼‰
                if _is_obvious_domain_spoofing(base_domain, safe_base):
                    site_description = safe_domains.get(safe_domain, "çŸ¥åç¶²ç«™")
                    return {
                        'is_spoofed': True,
                        'original_domain': safe_domain,
                        'spoofed_domain': domain,
                        'spoofing_type': "ç¶²åŸŸåç¨±è®Šå½¢æ”»æ“Š",
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} ç–‘ä¼¼æ¨¡ä»¿æ­£ç‰Œçš„ {safe_domain} ({site_description})ã€‚\n\nè©é¨™é›†åœ˜å¸¸ç”¨ç›¸ä¼¼çš„ç¶²åŸŸåç¨±ä¾†æ··æ·†è¦–è½ï¼Œè£½ä½œå‡ç¶²ç«™é¨™å–å€‹äººè³‡æ–™æˆ–ä¿¡ç”¨å¡è³‡è¨Šã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
            
            # æª¢æŸ¥æ¯å€‹ç™½åå–®ç¶²åŸŸæ˜¯å¦æœ‰ç›¸ä¼¼æ€§
            similar_domains_advanced = []
            
            for safe_domain in safe_domains.keys():
                safe_domain_lower = safe_domain.lower()
                safe_domain_without_www = safe_domain_lower[4:] if safe_domain_lower.startswith('www.') else safe_domain_lower
                
                # è·³éå®Œå…¨ç›¸åŒçš„ç¶²åŸŸï¼ˆåŒ…å«wwwè®Šé«”ï¼‰
                if (domain == safe_domain_lower or 
                    domain == safe_domain_without_www or 
                    domain_without_www == safe_domain_lower or 
                    domain_without_www == safe_domain_without_www):
                    continue
                
                # åªæª¢æ¸¬é«˜ç›¸ä¼¼åº¦çš„è®Šå½¢ï¼ˆé¿å…èª¤å ±ï¼‰
                # éœ€è¦åŸŸåé•·åº¦ç›¸è¿‘ï¼ˆå·®è·ä¸è¶…é2å€‹å­—ç¬¦ï¼‰ä¸”æœ‰é«˜åº¦ç›¸ä¼¼æ€§
                length_diff = abs(len(domain_without_www) - len(safe_domain_without_www))
                if length_diff > 2:
                    continue  # é•·åº¦å·®è·å¤ªå¤§ï¼Œè·³é
                
                # é¡å¤–æª¢æŸ¥ï¼šç¶²åŸŸåç¨±å¿…é ˆæœ‰è¶³å¤ çš„ç›¸ä¼¼æ€§
                if not _has_sufficient_similarity(domain_without_www, safe_domain_without_www):
                    continue  # ç›¸ä¼¼åº¦ä¸è¶³ï¼Œè·³é
                
                # åªæª¢æ¸¬éå¸¸ç›¸ä¼¼çš„ç¶²åŸŸ
                spoofing_detected = False
                spoofing_type = ""
                
                # 1. å­—å…ƒæ›¿æ›æ”»æ“Šï¼ˆåªæª¢æ¸¬1å€‹å­—ç¬¦çš„å·®ç•°ï¼‰
                if _is_character_substitution(domain_without_www, safe_domain_without_www, max_substitutions=1):
                    spoofing_detected = True
                    spoofing_type = "å­—å…ƒæ›¿æ›"
                
                # 2. æ’å…¥é¡å¤–å­—å…ƒï¼ˆåªæª¢æ¸¬1å€‹å­—ç¬¦çš„æ’å…¥ï¼‰
                elif _is_character_insertion(domain_without_www, safe_domain_without_www, max_insertions=1):
                    spoofing_detected = True
                    spoofing_type = "æ’å…¥é¡å¤–å­—å…ƒ"
                
                # 3. ç›¸ä¼¼å­—å…ƒæ”»æ“Šï¼ˆåœ‹éš›åŒ–åŸŸåæ”»æ“Šï¼‰
                elif _is_homograph_attack(domain_without_www, safe_domain_without_www):
                    spoofing_detected = True
                    spoofing_type = "ç›¸ä¼¼å­—å…ƒæ”»æ“Š"
                
                if spoofing_detected:
                    site_description = safe_domains.get(safe_domain, "çŸ¥åç¶²ç«™")
                    similar_domains_advanced.append({
                        'domain': safe_domain,
                        'description': site_description,
                        'type': spoofing_type
                    })
            
            # å¦‚æœæ‰¾åˆ°å¤šå€‹ç›¸ä¼¼åŸŸåï¼Œæä¾›å¤šé‡è­¦å‘Š
            if len(similar_domains_advanced) > 0:
                if len(similar_domains_advanced) == 1:
                    # å–®ä¸€ç›¸ä¼¼åŸŸå
                    similar_domain = similar_domains_advanced[0]
                    return {
                        'is_spoofed': True,
                        'original_domain': similar_domain['domain'],
                        'spoofed_domain': domain,
                        'spoofing_type': similar_domain['type'],
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} ç–‘ä¼¼æ¨¡ä»¿æ­£ç‰Œçš„ {similar_domain['domain']} ({similar_domain['description']})ã€‚\n\nè©é¨™é›†åœ˜å¸¸ç”¨é€™ç¨®æ‰‹æ³•è£½ä½œå‡ç¶²ç«™ä¾†é¨™å–å€‹äººè³‡æ–™æˆ–ä¿¡ç”¨å¡è³‡è¨Šã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
                else:
                    # å¤šé‡ç›¸ä¼¼åŸŸå - é™åˆ¶æœ€å¤šé¡¯ç¤º3å€‹
                    domain_list = []
                    # åªå–å‰3å€‹ç›¸ä¼¼åŸŸå
                    for similar in similar_domains_advanced[:3]:
                        domain_list.append(f"{similar['domain']}({similar['description']})")
                    
                    # å¦‚æœé‚„æœ‰æ›´å¤šç›¸ä¼¼åŸŸåï¼Œæ·»åŠ æç¤º
                    if len(similar_domains_advanced) > 3:
                        domain_list.append(f"...ç­‰å…±{len(similar_domains_advanced)}å€‹ç›¸ä¼¼ç¶²åŸŸ")
                    
                    return {
                        'is_spoofed': True,
                        'original_domain': 'multiple',
                        'spoofed_domain': domain,
                        'spoofing_type': "å¤šé‡è®Šå½¢æ”»æ“Š",
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} å¯èƒ½æ¨¡ä»¿å¤šå€‹æ­£ç‰Œç¶²åŸŸï¼š\n\n" + "\n".join([f"â€¢ {d}" for d in domain_list]) + f"\n\né€™æ˜¯å€‹å¯ç–‘çš„ç¶²åŸŸï¼Œè©é¨™é›†åœ˜å¸¸ç”¨ç›¸ä¼¼çš„ç¶²åŸŸåç¨±ä¾†æ··æ·†è¦–è½ï¼Œè£½ä½œå‡ç¶²ç«™é¨™å–å€‹äººè³‡æ–™æˆ–ä¿¡ç”¨å¡è³‡è¨Šã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
        
        except Exception as e:
            # URLè§£æå¤±æ•—ï¼Œç¹¼çºŒæª¢æŸ¥ä¸‹ä¸€å€‹
            continue
    
    return {'is_spoofed': False}

def _is_legitimate_variant(domain, safe_domain, all_safe_domains):
    """æª¢æŸ¥æ˜¯å¦ç‚ºåˆæ³•çš„ç¶²åŸŸè®Šé«”ï¼Œé¿å…èª¤å ±"""
    # å°‡æ‰€æœ‰å®‰å…¨ç¶²åŸŸè½‰ç‚ºå°å¯«åˆ—è¡¨
    safe_domain_list = [d.lower() for d in all_safe_domains.keys()]
    
    # å¦‚æœæª¢æ¸¬çš„ç¶²åŸŸæœ¬èº«å°±åœ¨ç™½åå–®ä¸­ï¼Œå‰‡ç‚ºåˆæ³•
    if domain in safe_domain_list:
        return True
    
    # å·²çŸ¥çš„åˆæ³•ç¶²åŸŸå°ï¼ˆé¿å…äº’ç›¸èª¤å ±ï¼‰
    legitimate_pairs = [
        ('google.com', 'google.com.tw'),
        ('pchome.com.tw', 'ithome.com.tw'),  # é€™å…©å€‹æ˜¯ä¸åŒçš„åˆæ³•ç¶²ç«™
        ('shopee.tw', 'shopee.com'),
        ('yahoo.com', 'yahoo.com.tw'),
        ('microsoft.com', 'office.com'),
        ('facebook.com', 'instagram.com'),  # åŒå…¬å¸ä½†ä¸åŒæœå‹™
    ]
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå·²çŸ¥çš„åˆæ³•å°
    for pair in legitimate_pairs:
        if (domain == pair[0] and safe_domain == pair[1]) or (domain == pair[1] and safe_domain == pair[0]):
            return True
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºåŒä¸€ç¶²ç«™çš„ä¸åŒé ‚ç´šåŸŸå
    domain_parts = domain.split('.')
    safe_parts = safe_domain.split('.')
    
    if len(domain_parts) >= 2 and len(safe_parts) >= 2:
        # æ¯”è¼ƒä¸»è¦ç¶²åŸŸåç¨±ï¼ˆå»é™¤é ‚ç´šåŸŸåï¼‰
        domain_main = '.'.join(domain_parts[:-1])
        safe_main = '.'.join(safe_parts[:-1])
        
        # å¦‚æœä¸»è¦éƒ¨åˆ†ç›¸åŒï¼Œå¯èƒ½æ˜¯åˆæ³•çš„åœ°å€è®Šé«”
        if domain_main == safe_main:
            return True
    
    return False

def _is_character_substitution(suspicious_domain, safe_domain, max_substitutions=2):
    """æª¢æ¸¬å­—å…ƒæ›¿æ›æ”»æ“Š - æ”¹é€²ç‰ˆ"""
    # è¨ˆç®—ç·¨è¼¯è·é›¢ï¼ˆLevenshtein distanceï¼‰
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    # æ”¹é€²çš„ç›¸ä¼¼åº¦æª¢æ¸¬
    distance = levenshtein_distance(suspicious_domain, safe_domain)
    length_diff = abs(len(suspicious_domain) - len(safe_domain))
    max_length = max(len(suspicious_domain), len(safe_domain))
    
    # å‹•æ…‹èª¿æ•´é–¾å€¼ï¼šè¼ƒçŸ­çš„ç¶²åŸŸå…è¨±è¼ƒå°‘çš„å·®ç•°
    if max_length <= 10:
        max_distance = 1
    elif max_length <= 15:
        max_distance = 2
    else:
        max_distance = 3
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå­—å…ƒæ›¿æ›æ”»æ“Š
    if distance <= max_distance and length_diff <= max_substitutions:
        # é¡å¤–æª¢æŸ¥ï¼šé¿å…èª¤åˆ¤å®Œå…¨ä¸ç›¸é—œçš„ç¶²åŸŸ
        # è¨ˆç®—æœ€é•·å…¬å…±å­åºåˆ—
        def lcs_length(s1, s2):
            m, n = len(s1), len(s2)
            dp = [[0] * (n + 1) for _ in range(m + 1)]
            
            for i in range(1, m + 1):
                for j in range(1, n + 1):
                    if s1[i-1] == s2[j-1]:
                        dp[i][j] = dp[i-1][j-1] + 1
                    else:
                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])
            
            return dp[m][n]
        
        lcs_len = lcs_length(suspicious_domain, safe_domain)
        similarity_ratio = lcs_len / max_length
        
        # è¦æ±‚è‡³å°‘70%çš„å­—å…ƒç›¸ä¼¼åº¦
        return similarity_ratio >= 0.7
    
    return False

def _is_character_insertion(suspicious_domain, safe_domain, max_insertions=2):
    """æª¢æ¸¬å­—å…ƒæ’å…¥æ”»æ“Š - æ”¹é€²ç‰ˆ"""
    # ç§»é™¤ www. å‰ç¶´
    if suspicious_domain.startswith('www.'):
        suspicious_domain = suspicious_domain[4:]
    if safe_domain.startswith('www.'):
        safe_domain = safe_domain[4:]
    
    # åˆ†è§£ç¶²åŸŸåç¨±
    safe_parts = safe_domain.split('.')
    suspicious_parts = suspicious_domain.split('.')
    
    # æª¢æŸ¥åŸºç¤ç¶²åŸŸåç¨±çš„æ’å…¥æ”»æ“Š
    safe_base = safe_parts[0]  # ä¾‹å¦‚ google, pchome, cht, amazon
    suspicious_base = suspicious_parts[0]  # ä¾‹å¦‚ google-search, pchome-24h, cht-tw, amazoner
    
    # 0. å„ªå…ˆæª¢æŸ¥æ˜¯å¦ç‚º-twæˆ–-taiwanå¾Œç¶´çš„è®Šå½¢æ”»æ“Šï¼ˆé«˜é¢¨éšªï¼‰
    if suspicious_base.endswith('-tw') or suspicious_base.endswith('-taiwan'):
        # æª¢æŸ¥å»é™¤å¾Œç¶´å¾Œæ˜¯å¦èˆ‡å®‰å…¨ç¶²åŸŸåŒ¹é…
        if suspicious_base.endswith('-tw'):
            base_without_suffix = suspicious_base[:-3]  # ç§»é™¤'-tw'
        else:  # '-taiwan'
            base_without_suffix = suspicious_base[:-8]  # ç§»é™¤'-taiwan'
        
        # æª¢æŸ¥å»é™¤å¾Œç¶´å¾Œæ˜¯å¦èˆ‡å®‰å…¨ç¶²åŸŸåŒ¹é…
        if base_without_suffix == safe_base:
            return True
    
    # 1. æª¢æŸ¥å­—æ¯å¾Œç¶´æ’å…¥ (amazon -> amazoner, google -> googles, facebook -> facebooker)
    if suspicious_base.startswith(safe_base) and len(suspicious_base) > len(safe_base):
        added_part = suspicious_base[len(safe_base):]
        # å¸¸è¦‹çš„å­—æ¯æ·»åŠ æ¨¡å¼
        common_letter_suffixes = ['r', 's', 'er', 'es', 'ing', 'ed', 'ly', 'al', 'ic', 'tion', 'ment']
        if added_part in common_letter_suffixes:
            return True
        # æª¢æŸ¥æ˜¯å¦åªæ˜¯æ·»åŠ äº†1-4å€‹å­—æ¯
        if len(added_part) <= 4 and added_part.isalpha():
            return True
    
    # 2. æª¢æŸ¥é€£å­—ç¬¦æ’å…¥ (google -> google-search, pchome -> pchome-24h)
    if '-' in suspicious_base and safe_base in suspicious_base:
        # ç§»é™¤é€£å­—ç¬¦å¾Œæª¢æŸ¥æ˜¯å¦åŒ…å«åŸå§‹ç¶²åŸŸ
        base_without_dash = suspicious_base.replace('-', '')
        if safe_base in base_without_dash:
            return True
        
        # æª¢æŸ¥é€£å­—ç¬¦å‰çš„éƒ¨åˆ†æ˜¯å¦èˆ‡å®‰å…¨ç¶²åŸŸåŒ¹é…
        dash_parts = suspicious_base.split('-')
        if dash_parts[0] == safe_base:
            return True
    
    # 3. æª¢æŸ¥æ•¸å­—æ’å…¥ (pchome -> pchome24h)
    import re
    # ç§»é™¤æ•¸å­—å¾Œæª¢æŸ¥
    base_without_numbers = re.sub(r'\d+', '', suspicious_base)
    if base_without_numbers == safe_base:
        return True
    
    # 4. æª¢æŸ¥å¸¸è¦‹å¾Œç¶´æ’å…¥
    common_suffixes = ['search', 'official', 'secure', 'login', 'bank', 'pay', 'tw', 'taiwan', '24h', 'shop', 'store', 'online', 'web', 'site', 'net', 'app', 'mobile']
    for suffix in common_suffixes:
        if suspicious_base == safe_base + suffix:
            return True
        if suspicious_base == safe_base + '-' + suffix:
            return True
    
    # 5. æª¢æŸ¥å­ç¶²åŸŸä¸­çš„è®Šå½¢æ”»æ“Š
    # ä¾‹å¦‚ event.liontravel-tw.com ä¸­çš„ liontravel-tw æ˜¯å° liontravel çš„è®Šå½¢
    if len(suspicious_parts) >= 2:
        for i, suspicious_part in enumerate(suspicious_parts):
            # æª¢æŸ¥æ¯å€‹éƒ¨åˆ†æ˜¯å¦åŒ…å«å°å®‰å…¨ç¶²åŸŸçš„è®Šå½¢
            if '-' in suspicious_part and safe_base in suspicious_part:
                # æª¢æŸ¥é€£å­—ç¬¦å‰çš„éƒ¨åˆ†æ˜¯å¦èˆ‡å®‰å…¨ç¶²åŸŸåŒ¹é…
                dash_parts = suspicious_part.split('-')
                if dash_parts[0] == safe_base:
                    return True
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå®‰å…¨ç¶²åŸŸåŠ ä¸Šå¸¸è¦‹å¾Œç¶´
            for suffix in common_suffixes:
                if suspicious_part == safe_base + '-' + suffix:
                    return True
                if suspicious_part == safe_base + suffix:
                    return True
            
            # æª¢æŸ¥å­—æ¯å¾Œç¶´æ’å…¥
            if suspicious_part.startswith(safe_base) and len(suspicious_part) > len(safe_base):
                added_part = suspicious_part[len(safe_base):]
                if len(added_part) <= 4 and added_part.isalpha():
                    return True
    
    # 6. æª¢æŸ¥å®Œæ•´ç¶²åŸŸçš„å­—æ¯æ’å…¥ (amazon.com -> amazoner.com)
    if len(safe_parts) == len(suspicious_parts):
        # æª¢æŸ¥æ¯å€‹éƒ¨åˆ†
        for safe_part, suspicious_part in zip(safe_parts, suspicious_parts):
            if suspicious_part.startswith(safe_part) and len(suspicious_part) > len(safe_part):
                added_part = suspicious_part[len(safe_part):]
                # å¦‚æœåªæ˜¯åœ¨æŸå€‹éƒ¨åˆ†å¾Œé¢åŠ äº†å­—æ¯
                if len(added_part) <= 4 and added_part.isalpha():
                    return True
    
    # 7. åŸæœ‰çš„æ¨¡å¼æª¢æ¸¬
    insertion_patterns = [
        f"{safe_domain.split('.')[0]}-tw.{'.'.join(safe_domain.split('.')[1:])}",
        f"{safe_domain.split('.')[0]}-taiwan.{'.'.join(safe_domain.split('.')[1:])}",
        f"{safe_domain}.tw",
        f"tw.{safe_domain}",
        f"taiwan.{safe_domain}",
    ]
    
    return suspicious_domain in insertion_patterns

def _is_domain_suffix_spoofing(suspicious_domain, safe_domain):
    """æª¢æ¸¬ç¶²åŸŸå¾Œç¶´è®Šå½¢"""
    # æª¢æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„å®‰å…¨ç¶²åŸŸä½œç‚ºå‰ç¶´
    if suspicious_domain.startswith(safe_domain + '.'):
        # ä¾‹å¦‚ cht.com.tw.fake.com åŒ…å« cht.com.tw.
        return True
    
    safe_base = safe_domain.split('.')[0]  # ä¾‹å¦‚å¾ google.com å–å¾— google
    suspicious_base = suspicious_domain.split('.')[0]  # ä¾‹å¦‚å¾ google-tw.com å–å¾— google-tw
    
    # æª¢æŸ¥æ˜¯å¦åœ¨åŸºç¤ç¶²åŸŸåç¨±å¾ŒåŠ äº†-twæˆ–-taiwanï¼ˆç›´æ¥åˆ¤å®šç‚ºé«˜é¢¨éšªï¼‰
    if suspicious_base == safe_base + '-tw' or suspicious_base == safe_base + '-taiwan':
        return True
    
    # æª¢æŸ¥æ˜¯å¦åœ¨åŸºç¤ç¶²åŸŸåç¨±å¾ŒåŠ äº†é¡å¤–å­—å…ƒ
    if suspicious_base.startswith(safe_base) and len(suspicious_base) > len(safe_base):
        added_part = suspicious_base[len(safe_base):]
        # å¸¸è¦‹çš„æ·»åŠ æ¨¡å¼
        common_additions = ['-tw', '-taiwan', '-official', '-secure', '-login', '-bank', '-pay']
        return any(added_part.startswith(addition) for addition in common_additions)
    
    return False

def _is_homograph_attack(suspicious_domain, safe_domain):
    """æª¢æ¸¬åŒéŸ³å­—æˆ–ç›¸ä¼¼å­—å…ƒæ”»æ“Š - æ”¹é€²ç‰ˆ"""
    # æ“´å±•çš„å­—å…ƒæ›¿æ›å°æ‡‰è¡¨
    homograph_map = {
        'o': ['0', 'Î¿', 'Ğ¾', 'Ã¸', 'Ã¶', 'Ã²', 'Ã³', 'Ã´', 'Ãµ'],  # è‹±æ–‡oåŠå„ç¨®è®Šå½¢
        'a': ['Ğ°', 'Î±', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', '@'],  # è‹±æ–‡aåŠå„ç¨®è®Šå½¢
        'e': ['Ğµ', 'Îµ', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', '3'],  # è‹±æ–‡eåŠå„ç¨®è®Šå½¢
        'i': ['1', 'l', 'Ñ–', 'Î¹', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', '!', '|'],  # è‹±æ–‡iåŠå„ç¨®è®Šå½¢
        'l': ['1', 'i', 'Î¹', '|', 'Å‚'],  # è‹±æ–‡låŠå„ç¨®è®Šå½¢
        'n': ['Î·', 'Ã±'],  # è‹±æ–‡nåŠå„ç¨®è®Šå½¢
        'p': ['Ï'],  # è‹±æ–‡påŠå„ç¨®è®Šå½¢
        'c': ['Ñ', 'Ã§', 'Â©'],  # è‹±æ–‡cåŠå„ç¨®è®Šå½¢
        'x': ['Ñ…', 'Ã—'],  # è‹±æ–‡xåŠå„ç¨®è®Šå½¢
        'u': ['Ï…', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼'],  # è‹±æ–‡uåŠå„ç¨®è®Šå½¢
        's': ['$', '5', 'ÅŸ'],  # è‹±æ–‡såŠå„ç¨®è®Šå½¢
        'g': ['9', 'ÄŸ'],  # è‹±æ–‡gåŠå„ç¨®è®Šå½¢
        't': ['7', 'Å£'],  # è‹±æ–‡tåŠå„ç¨®è®Šå½¢
        'b': ['6', 'Î²'],  # è‹±æ–‡båŠå„ç¨®è®Šå½¢
        'd': ['Ã°'],  # è‹±æ–‡dåŠå„ç¨®è®Šå½¢
        'f': ['Æ’'],  # è‹±æ–‡fåŠå„ç¨®è®Šå½¢
        'h': ['Ä§'],  # è‹±æ–‡håŠå„ç¨®è®Šå½¢
        'k': ['Îº'],  # è‹±æ–‡kåŠå„ç¨®è®Šå½¢
        'm': ['Î¼'],  # è‹±æ–‡måŠå„ç¨®è®Šå½¢
        'r': ['Ï', 'Å™'],  # è‹±æ–‡råŠå„ç¨®è®Šå½¢
        'v': ['Î½'],  # è‹±æ–‡våŠå„ç¨®è®Šå½¢
        'w': ['Ï‰'],  # è‹±æ–‡wåŠå„ç¨®è®Šå½¢
        'y': ['Ã½', 'Ã¿'],  # è‹±æ–‡yåŠå„ç¨®è®Šå½¢
        'z': ['Î¶', 'Å¾'],  # è‹±æ–‡zåŠå„ç¨®è®Šå½¢
    }
    
    # æª¢æŸ¥æ¯å€‹å­—å…ƒæ˜¯å¦è¢«æ›¿æ›
    if len(suspicious_domain) != len(safe_domain):
        return False
    
    substitution_count = 0
    for i, (sus_char, safe_char) in enumerate(zip(suspicious_domain, safe_domain)):
        if sus_char != safe_char:
            # æª¢æŸ¥æ˜¯å¦ç‚ºå·²çŸ¥çš„ç›¸ä¼¼å­—å…ƒæ›¿æ›
            if safe_char.lower() in homograph_map:
                if sus_char in homograph_map[safe_char.lower()]:
                    substitution_count += 1
                else:
                    return False  # ä¸æ˜¯å·²çŸ¥çš„ç›¸ä¼¼å­—å…ƒæ›¿æ›
            else:
                return False  # å­—å…ƒä¸åœ¨æ›¿æ›è¡¨ä¸­
    
    # å¦‚æœæœ‰1-3å€‹å­—å…ƒè¢«æ›¿æ›ï¼Œèªç‚ºæ˜¯ç›¸ä¼¼å­—å…ƒæ”»æ“Š
    return 1 <= substitution_count <= 3

def _is_obvious_domain_spoofing(suspicious_base, safe_base):
    """æª¢æŸ¥æ˜é¡¯çš„ç¶²åŸŸåç¨±è®Šå½¢æ”»æ“Š"""
    # å¦‚æœå®Œå…¨ç›¸åŒï¼Œä¸æ˜¯è®Šå½¢æ”»æ“Š
    if suspicious_base == safe_base:
        return False
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«åŸå§‹ç¶²åŸŸåç¨±ä½œç‚ºå­å­—ä¸²
    if safe_base in suspicious_base:
        # æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹çš„è®Šå½¢æ¨¡å¼
        
        # 1. æ’å…¥é€£å­—ç¬¦å’Œé¡å¤–å­—å…ƒ (fetc -> fetc-nete, google -> google-search)
        if suspicious_base.startswith(safe_base + '-'):
            return True
        
        # 2. åœ¨ä¸­é–“æ’å…¥å­—å…ƒ (fetc -> fetc-nete, amazon -> amazoner)
        if suspicious_base.startswith(safe_base):
            added_part = suspicious_base[len(safe_base):]
            # å¦‚æœæ·»åŠ çš„éƒ¨åˆ†æ˜¯å¸¸è¦‹çš„è®Šå½¢æ¨¡å¼
            if len(added_part) <= 6 and (added_part.startswith('-') or added_part.isalpha()):
                return True
        
        # 3. æª¢æŸ¥æ˜¯å¦åœ¨åŸå§‹ç¶²åŸŸä¸­é–“æ’å…¥å­—å…ƒ
        # ä¾‹å¦‚ fetc -> fe-tc, f-etc ç­‰
        for i in range(1, len(safe_base)):
            prefix = safe_base[:i]
            suffix = safe_base[i:]
            # æª¢æŸ¥æ˜¯å¦ç‚º prefix + æ’å…¥å­—å…ƒ + suffix çš„æ¨¡å¼
            if suspicious_base.startswith(prefix) and suspicious_base.endswith(suffix):
                middle_part = suspicious_base[len(prefix):-len(suffix)] if suffix else suspicious_base[len(prefix):]
                if len(middle_part) <= 4 and ('-' in middle_part or middle_part.isalpha()):
                    return True
    
    # 4. æª¢æŸ¥å­—å…ƒæ›¿æ›æ”»æ“Šï¼ˆæ›´å¯¬é¬†çš„æ¢ä»¶ï¼‰
    if abs(len(suspicious_base) - len(safe_base)) <= 2:
        # è¨ˆç®—ç·¨è¼¯è·é›¢
        def levenshtein_distance(s1, s2):
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        distance = levenshtein_distance(suspicious_base, safe_base)
        max_length = max(len(suspicious_base), len(safe_base))
        
        # å¦‚æœç·¨è¼¯è·é›¢å°æ–¼ç­‰æ–¼2ï¼Œä¸”æœ‰è¶³å¤ çš„ç›¸ä¼¼æ€§
        if distance <= 2:
            # è¨ˆç®—æœ€é•·å…¬å…±å­åºåˆ—
            def lcs_length(s1, s2):
                m, n = len(s1), len(s2)
                dp = [[0] * (n + 1) for _ in range(m + 1)]
                
                for i in range(1, m + 1):
                    for j in range(1, n + 1):
                        if s1[i-1] == s2[j-1]:
                            dp[i][j] = dp[i-1][j-1] + 1
                        else:
                            dp[i][j] = max(dp[i-1][j], dp[i][j-1])
                
                return dp[m][n]
            
            lcs_len = lcs_length(suspicious_base, safe_base)
            similarity_ratio = lcs_len / max_length
            
            # é™ä½é–¾å€¼åˆ°60%ï¼Œå°ˆé–€é‡å°æ˜é¡¯çš„è®Šå½¢æ”»æ“Š
            if similarity_ratio >= 0.6:
                return True
    
    return False

def _has_sufficient_similarity(domain1, domain2):
    """æª¢æŸ¥å…©å€‹ç¶²åŸŸæ˜¯å¦æœ‰è¶³å¤ çš„ç›¸ä¼¼æ€§"""
    # è¨ˆç®—æœ€é•·å…¬å…±å­åºåˆ—
    def lcs_length(s1, s2):
        m, n = len(s1), len(s2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if s1[i-1] == s2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    # è¨ˆç®—ç›¸ä¼¼åº¦æ¯”ä¾‹
    max_length = max(len(domain1), len(domain2))
    lcs_len = lcs_length(domain1, domain2)
    similarity_ratio = lcs_len / max_length
    
    # è¦æ±‚è‡³å°‘80%çš„å­—å…ƒç›¸ä¼¼åº¦
    return similarity_ratio >= 0.8 

def _detect_government_domain_spoofing(domain):
    """
    å°ˆé–€æª¢æ¸¬æ”¿åºœç¶²åŸŸè®Šå½¢æ”»æ“Š
    
    Args:
        domain: è¦æª¢æ¸¬çš„åŸŸåï¼ˆå·²ç§»é™¤wwwå‰ç¶´ï¼‰
        
    Returns:
        dict: æª¢æ¸¬çµæœ
    """
    # é¦–å…ˆæª¢æŸ¥æ˜¯å¦æ˜¯æ­£ç¢ºçš„æ”¿åºœç¶²åŸŸæ ¼å¼
    if domain.endswith('.gov.tw'):
        return {'is_spoofed': False}  # æ­£ç¢ºçš„æ”¿åºœç¶²åŸŸæ ¼å¼ï¼Œä¸æ˜¯è©é¨™
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯é ‚ç´šçš„gov.twåŸŸåï¼ˆé€™æ˜¯åˆæ³•çš„ï¼‰
    if domain == 'gov.tw':
        return {'is_spoofed': False}  # gov.twæœ¬èº«æ˜¯åˆæ³•çš„
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å« "gov" ä½†ä¸æ˜¯æ­£ç¢ºçš„æ ¼å¼
    has_gov = 'gov' in domain.lower()
    
    if has_gov:
        # å¸¸è¦‹çš„æ”¿åºœç¶²åŸŸè®Šå½¢æ¨¡å¼
        suspicious_patterns = [
            '-gov.com',  # example-gov.com
            'gov.com',   # somegovdepartment.com  
            'gov.net',   # gov.net
            'gov.org',   # gov.org
            '.com/tw',   # å¦‚ cdic-gov.com/tw
            '-gov.net',  # example-gov.net
            'gov-',      # gov-something.com
            'govtw',     # govtw.com
            'gov.tw.',   # gov.tw.fake.com
        ]
        
        # æª¢æŸ¥æ˜¯å¦åŒ¹é…å¯ç–‘æ¨¡å¼
        for pattern in suspicious_patterns:
            if pattern in domain.lower():
                return {
                    'is_spoofed': True,
                    'original_domain': 'gov.tw',
                    'spoofed_domain': domain,
                    'spoofing_type': 'æ”¿åºœç¶²åŸŸè®Šå½¢æ”»æ“Š',
                    'risk_explanation': f"ğŸš¨ æ¥µé«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ã€Œ{domain}ã€æ˜¯å‡å†’æ”¿åºœç¶²ç«™çš„è©é¨™ç¶²å€ï¼\n\nâœ… çœŸæ­£çš„æ”¿åºœç¶²ç«™éƒ½æ˜¯ä»¥ã€Œ.gov.twã€çµå°¾\nâŒ é€™å€‹ç¶²å€ä½¿ç”¨äº†éŒ¯èª¤çš„æ ¼å¼ä¾†å†’å……æ”¿åºœæ©Ÿé—œ\n\nç¶²é æ¨™é¡Œäº‚ç¢¼é¡¯ç¤ºï¼Œå‡å†’æ”¿åºœæ©Ÿé—œè©é¨™ã€‚æ”¿åºœæ©Ÿé—œä¸æœƒé€éLINEè¦æ±‚å€‹è³‡æˆ–æ“ä½œATMã€‚\n\nğŸ›¡ï¸ è«‹è¨˜ä½ï¼šæ”¿åºœæ©Ÿé—œçš„å®˜æ–¹ç¶²ç«™ä¸€å¾‹ä»¥ã€Œ.gov.twã€çµå°¾ï¼Œä»»ä½•å…¶ä»–æ ¼å¼éƒ½æ˜¯å‡çš„ï¼\n\nğŸ“ å¦‚æœ‰ç–‘å•è«‹æ’¥æ‰“165åè©é¨™å°ˆç·šç¢ºèª"
                }
        
        # å¦‚æœåŒ…å«govä½†æ²’æœ‰åŒ¹é…åˆ°ç‰¹å®šæ¨¡å¼ï¼Œçµ¦å‡ºè¼ƒæº«å’Œçš„è­¦å‘Š
        # ä½†è¦æ’é™¤ä¸€äº›å¯èƒ½çš„åˆæ³•æƒ…æ³
        if not any(legitimate in domain.lower() for legitimate in ['google', 'govtech', 'government']):
            return {
                'is_spoofed': True,
                'original_domain': 'gov.tw',
                'spoofed_domain': domain,
                'spoofing_type': 'ç–‘ä¼¼æ”¿åºœç¶²åŸŸè®Šå½¢æ”»æ“Š',
                'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ã€Œ{domain}ã€åŒ…å«ã€Œgovã€å­—æ¨£ä½†ä¸æ˜¯æ­£ç¢ºçš„æ”¿åºœç¶²ç«™æ ¼å¼ï¼\n\nâœ… çœŸæ­£çš„æ”¿åºœç¶²ç«™éƒ½æ˜¯ä»¥ã€Œ.gov.twã€çµå°¾\nâŒ é€™å€‹ç¶²å€æ ¼å¼å¯ç–‘ï¼Œå¯èƒ½æ˜¯è©é¨™ç¶²ç«™\n\nç¶²é æ¨™é¡Œäº‚ç¢¼é¡¯ç¤ºï¼Œå‡å†’æ”¿åºœæ©Ÿé—œè©é¨™ã€‚æ”¿åºœæ©Ÿé—œä¸æœƒé€éLINEè¦æ±‚å€‹è³‡æˆ–æ“ä½œATMã€‚\n\nè«‹å‹™å¿…ç¢ºèªï¼š\nâ€¢ æ”¿åºœæ©Ÿé—œå®˜ç¶²ä¸€å¾‹ä»¥ã€Œ.gov.twã€çµå°¾\nâ€¢ å¦‚éœ€è¾¦ç†æ”¿åºœæ¥­å‹™ï¼Œè«‹ç›´æ¥åˆ°å®˜æ–¹ç¶²ç«™\nâ€¢ ä¸è¦åœ¨å¯ç–‘ç¶²ç«™è¼¸å…¥å€‹äººè³‡æ–™\n\nğŸ“ å¦‚æœ‰ç–‘å•è«‹æ’¥æ‰“165åè©é¨™å°ˆç·šç¢ºèª"
            }
    
    return {'is_spoofed': False} 


def _is_legitimate_domain_variant(suspicious_domain, safe_domain):
    """
    æª¢æŸ¥æ˜¯å¦ç‚ºåˆæ³•çš„åŸŸåè®Šé«”
    
    Args:
        suspicious_domain: å¯ç–‘åŸŸåï¼ˆå¦‚ cht.twï¼‰
        safe_domain: å®‰å…¨åŸŸåï¼ˆå¦‚ cht.com.twï¼‰
        
    Returns:
        bool: æ˜¯å¦ç‚ºåˆæ³•è®Šé«”
    """
    # æå–åŸºç¤åŸŸå
    suspicious_parts = suspicious_domain.split('.')
    safe_parts = safe_domain.split('.')
    
    suspicious_base = suspicious_parts[0]
    safe_base = safe_parts[0]
    
    # å¦‚æœåŸºç¤åŸŸåä¸åŒï¼Œä¸æ˜¯è®Šé«”
    if suspicious_base != safe_base:
        return False
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºåˆæ³•çš„çŸ­åŸŸåè®Šé«”
    # ä¾‹å¦‚ï¼šcht.tw æ˜¯ cht.com.tw çš„åˆæ³•è®Šé«”
    if len(suspicious_parts) == 2 and len(safe_parts) == 3:
        # æª¢æŸ¥æ˜¯å¦ç‚º .tw å° .com.tw çš„è®Šé«”
        if (suspicious_parts[1] == 'tw' and 
            safe_parts[1] == 'com' and 
            safe_parts[2] == 'tw'):
            return True
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå…¶ä»–å¸¸è¦‹çš„åˆæ³•è®Šé«”
    # ä¾‹å¦‚ï¼šexample.com å’Œ example.net éƒ½æ˜¯åŒä¸€æ©Ÿæ§‹çš„åˆæ³•åŸŸå
    legitimate_variants = [
        # å°ç£åŸŸåè®Šé«”
        ('.tw', '.com.tw'),
        ('.tw', '.org.tw'),
        ('.tw', '.net.tw'),
        # åœ‹éš›åŸŸåè®Šé«”
        ('.com', '.net'),
        ('.com', '.org'),
        ('.net', '.com'),
        ('.org', '.com'),
    ]
    
    for variant in legitimate_variants:
        if (suspicious_domain.endswith(variant[0]) and 
            safe_domain.endswith(variant[1])):
            return True
    
    return False
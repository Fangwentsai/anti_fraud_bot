"""
ç¶²åŸŸè®Šå½¢æ”»æ“Šæª¢æ¸¬æ¨¡çµ„
ç”¨æ–¼æª¢æ¸¬æ¨¡ä»¿ç™½åå–®ç¶²åŸŸçš„å¯ç–‘ç¶²å€
"""

import re
from urllib.parse import urlparse

def detect_domain_spoofing(url_or_message, safe_domains):
    """
    æª¢æ¸¬ç¶²åŸŸè®Šå½¢æ”»æ“Š - è­˜åˆ¥æ¨¡ä»¿ç™½åå–®ç¶²åŸŸçš„å¯ç–‘ç¶²å€
    
    Args:
        url_or_message: è¦æª¢æ¸¬çš„URLæˆ–åŒ…å«URLçš„è¨Šæ¯
        safe_domains: ç™½åå–®ç¶²åŸŸå­—å…¸
        
    Returns:
        dict: {
            'is_spoofed': bool,  # æ˜¯å¦ç‚ºè®Šå½¢ç¶²åŸŸ
            'original_domain': str,  # è¢«æ¨¡ä»¿çš„åŸå§‹ç¶²åŸŸ
            'spoofed_domain': str,  # å¯ç–‘çš„è®Šå½¢ç¶²åŸŸ
            'spoofing_type': str,  # è®Šå½¢é¡å‹
            'risk_explanation': str  # é¢¨éšªèªªæ˜
        }
    """
    # æå–URL
    url_pattern = re.compile(r'https?://[^\s]+|www\.[^\s]+|[a-zA-Z0-9][a-zA-Z0-9-]*\.[a-zA-Z]{2,}[^\s]*')
    urls = url_pattern.findall(url_or_message)
    
    if not urls:
        return {'is_spoofed': False}
    
    for url in urls:
        # æ¨™æº–åŒ–URL
        if not url.startswith(('http://', 'https://')):
            if url.startswith('www.'):
                url = 'https://' + url
            else:
                url = 'https://' + url
        
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ www. å‰ç¶´é€²è¡Œæ¯”è¼ƒ
            domain_without_www = domain[4:] if domain.startswith('www.') else domain
            
            # å‰µå»ºæ¨™æº–åŒ–çš„å®‰å…¨ç¶²åŸŸåˆ—è¡¨ï¼ˆåŒ…å«wwwå’Œéwwwç‰ˆæœ¬ï¼‰
            normalized_safe_domains = set()
            for safe_domain in safe_domains.keys():
                safe_domain_lower = safe_domain.lower()
                normalized_safe_domains.add(safe_domain_lower)
                
                # æ·»åŠ wwwå’Œéwwwç‰ˆæœ¬
                if safe_domain_lower.startswith('www.'):
                    normalized_safe_domains.add(safe_domain_lower[4:])
                else:
                    normalized_safe_domains.add('www.' + safe_domain_lower)
            
            # æª¢æŸ¥æ˜¯å¦æœ¬èº«å°±æ˜¯ç™½åå–®ç¶²åŸŸï¼ˆåŒ…å«wwwè®Šé«”ï¼‰
            if domain in normalized_safe_domains or domain_without_www in normalized_safe_domains:
                continue  # é€™æ˜¯æ­£å¸¸çš„ç™½åå–®ç¶²åŸŸï¼Œè·³é
            
            # æª¢æŸ¥æ¯å€‹ç™½åå–®ç¶²åŸŸ
            for safe_domain in safe_domains.keys():
                safe_domain_lower = safe_domain.lower()
                safe_domain_without_www = safe_domain_lower[4:] if safe_domain_lower.startswith('www.') else safe_domain_lower
                
                # è·³éå®Œå…¨ç›¸åŒçš„ç¶²åŸŸï¼ˆåŒ…å«wwwè®Šé«”ï¼‰
                if (domain == safe_domain_lower or 
                    domain == safe_domain_without_www or 
                    domain_without_www == safe_domain_lower or 
                    domain_without_www == safe_domain_without_www):
                    continue
                
                # è·³éåˆæ³•çš„å­ç¶²åŸŸï¼ˆä¾‹å¦‚ mail.google.com, maps.google.comï¼‰
                if (domain.endswith('.' + safe_domain_lower) or 
                    domain.endswith('.' + safe_domain_without_www)):
                    continue
                
                # è·³éå·²çŸ¥çš„åˆæ³•è®Šé«”ï¼ˆé¿å…èª¤å ±ï¼‰
                if _is_legitimate_variant(domain_without_www, safe_domain_without_www, safe_domains):
                    continue
                
                # æª¢æ¸¬å„ç¨®è®Šå½¢æ‰‹æ³•ï¼ˆä½¿ç”¨å»é™¤wwwçš„ç‰ˆæœ¬é€²è¡Œæ¯”è¼ƒï¼‰
                spoofing_detected = False
                spoofing_type = ""
                
                # 1. å­—å…ƒæ›¿æ›æ”»æ“Š (ä¾‹å¦‚ google.com -> goog1e.com, googlĞµ.com)
                if _is_character_substitution(domain_without_www, safe_domain_without_www):
                    spoofing_detected = True
                    spoofing_type = "å­—å…ƒæ›¿æ›"
                
                # 2. æ’å…¥é¡å¤–å­—å…ƒ (ä¾‹å¦‚ google.com -> google-tw.com, google.com.tw)
                elif _is_character_insertion(domain_without_www, safe_domain_without_www):
                    spoofing_detected = True
                    spoofing_type = "æ’å…¥é¡å¤–å­—å…ƒ"
                
                # 3. ç¶²åŸŸå¾Œç¶´è®Šå½¢ (ä¾‹å¦‚ google.com -> google.com.tw, google-tw.com)
                elif _is_domain_suffix_spoofing(domain_without_www, safe_domain_without_www):
                    spoofing_detected = True
                    spoofing_type = "ç¶²åŸŸå¾Œç¶´è®Šå½¢"
                
                # 4. åŒéŸ³å­—æˆ–ç›¸ä¼¼å­—æ”»æ“Š (ä¾‹å¦‚ google.com -> goog1e.com)
                elif _is_homograph_attack(domain_without_www, safe_domain_without_www):
                    spoofing_detected = True
                    spoofing_type = "ç›¸ä¼¼å­—å…ƒæ”»æ“Š"
                
                if spoofing_detected:
                    site_description = safe_domains.get(safe_domain, "çŸ¥åç¶²ç«™")
                    return {
                        'is_spoofed': True,
                        'original_domain': safe_domain,
                        'spoofed_domain': domain,
                        'spoofing_type': spoofing_type,
                        'risk_explanation': f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼\n\né€™å€‹ç¶²å€ {domain} ç–‘ä¼¼æ¨¡ä»¿æ­£ç‰Œçš„ {safe_domain} ({site_description})ã€‚\n\nè©é¨™é›†åœ˜å¸¸ç”¨é€™ç¨®æ‰‹æ³•è£½ä½œå‡ç¶²ç«™ä¾†é¨™å–å€‹äººè³‡æ–™æˆ–ä¿¡ç”¨å¡è³‡è¨Šã€‚\n\nğŸš¨ åƒè¬ä¸è¦åœ¨é€™å€‹ç¶²ç«™è¼¸å…¥ä»»ä½•å€‹äººè³‡æ–™ã€å¯†ç¢¼æˆ–ä¿¡ç”¨å¡è™Ÿç¢¼ï¼"
                    }
        
        except Exception as e:
            # URLè§£æå¤±æ•—ï¼Œç¹¼çºŒæª¢æŸ¥ä¸‹ä¸€å€‹
            continue
    
    return {'is_spoofed': False}

def _is_legitimate_variant(domain, safe_domain, all_safe_domains):
    """æª¢æŸ¥æ˜¯å¦ç‚ºåˆæ³•çš„ç¶²åŸŸè®Šé«”ï¼Œé¿å…èª¤å ±"""
    # å°‡æ‰€æœ‰å®‰å…¨ç¶²åŸŸè½‰ç‚ºå°å¯«åˆ—è¡¨
    safe_domain_list = [d.lower() for d in all_safe_domains.keys()]
    
    # å¦‚æœæª¢æ¸¬çš„ç¶²åŸŸæœ¬èº«å°±åœ¨ç™½åå–®ä¸­ï¼Œå‰‡ç‚ºåˆæ³•
    if domain in safe_domain_list:
        return True
    
    # å·²çŸ¥çš„åˆæ³•ç¶²åŸŸå°ï¼ˆé¿å…äº’ç›¸èª¤å ±ï¼‰
    legitimate_pairs = [
        ('google.com', 'google.com.tw'),
        ('pchome.com.tw', 'ithome.com.tw'),  # é€™å…©å€‹æ˜¯ä¸åŒçš„åˆæ³•ç¶²ç«™
        ('shopee.tw', 'shopee.com'),
        ('yahoo.com', 'yahoo.com.tw'),
        ('microsoft.com', 'office.com'),
        ('facebook.com', 'instagram.com'),  # åŒå…¬å¸ä½†ä¸åŒæœå‹™
    ]
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå·²çŸ¥çš„åˆæ³•å°
    for pair in legitimate_pairs:
        if (domain == pair[0] and safe_domain == pair[1]) or (domain == pair[1] and safe_domain == pair[0]):
            return True
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºåŒä¸€ç¶²ç«™çš„ä¸åŒé ‚ç´šåŸŸå
    domain_parts = domain.split('.')
    safe_parts = safe_domain.split('.')
    
    if len(domain_parts) >= 2 and len(safe_parts) >= 2:
        # æ¯”è¼ƒä¸»è¦ç¶²åŸŸåç¨±ï¼ˆå»é™¤é ‚ç´šåŸŸåï¼‰
        domain_main = '.'.join(domain_parts[:-1])
        safe_main = '.'.join(safe_parts[:-1])
        
        # å¦‚æœä¸»è¦éƒ¨åˆ†ç›¸åŒï¼Œå¯èƒ½æ˜¯åˆæ³•çš„åœ°å€è®Šé«”
        if domain_main == safe_main:
            return True
    
    return False

def _is_character_substitution(suspicious_domain, safe_domain):
    """æª¢æ¸¬å­—å…ƒæ›¿æ›æ”»æ“Š - æ”¹é€²ç‰ˆ"""
    # è¨ˆç®—ç·¨è¼¯è·é›¢ï¼ˆLevenshtein distanceï¼‰
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    # æ”¹é€²çš„ç›¸ä¼¼åº¦æª¢æ¸¬
    distance = levenshtein_distance(suspicious_domain, safe_domain)
    length_diff = abs(len(suspicious_domain) - len(safe_domain))
    max_length = max(len(suspicious_domain), len(safe_domain))
    
    # å‹•æ…‹èª¿æ•´é–¾å€¼ï¼šè¼ƒçŸ­çš„ç¶²åŸŸå…è¨±è¼ƒå°‘çš„å·®ç•°
    if max_length <= 10:
        max_distance = 1
    elif max_length <= 15:
        max_distance = 2
    else:
        max_distance = 3
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå­—å…ƒæ›¿æ›æ”»æ“Š
    if distance <= max_distance and length_diff <= 1:
        # é¡å¤–æª¢æŸ¥ï¼šé¿å…èª¤åˆ¤å®Œå…¨ä¸ç›¸é—œçš„ç¶²åŸŸ
        # è¨ˆç®—æœ€é•·å…¬å…±å­åºåˆ—
        def lcs_length(s1, s2):
            m, n = len(s1), len(s2)
            dp = [[0] * (n + 1) for _ in range(m + 1)]
            
            for i in range(1, m + 1):
                for j in range(1, n + 1):
                    if s1[i-1] == s2[j-1]:
                        dp[i][j] = dp[i-1][j-1] + 1
                    else:
                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])
            
            return dp[m][n]
        
        lcs_len = lcs_length(suspicious_domain, safe_domain)
        similarity_ratio = lcs_len / max_length
        
        # è¦æ±‚è‡³å°‘70%çš„å­—å…ƒç›¸ä¼¼åº¦
        return similarity_ratio >= 0.7
    
    return False

def _is_character_insertion(suspicious_domain, safe_domain):
    """æª¢æ¸¬å­—å…ƒæ’å…¥æ”»æ“Š - æ”¹é€²ç‰ˆ"""
    # ç§»é™¤ www. å‰ç¶´
    if suspicious_domain.startswith('www.'):
        suspicious_domain = suspicious_domain[4:]
    if safe_domain.startswith('www.'):
        safe_domain = safe_domain[4:]
    
    # åˆ†è§£ç¶²åŸŸåç¨±
    safe_parts = safe_domain.split('.')
    suspicious_parts = suspicious_domain.split('.')
    
    # æª¢æŸ¥åŸºç¤ç¶²åŸŸåç¨±çš„æ’å…¥æ”»æ“Š
    safe_base = safe_parts[0]  # ä¾‹å¦‚ google, pchome, cht
    suspicious_base = suspicious_parts[0]  # ä¾‹å¦‚ google-search, pchome-24h, cht-tw
    
    # 1. æª¢æŸ¥é€£å­—ç¬¦æ’å…¥ (google -> google-search, pchome -> pchome-24h)
    if '-' in suspicious_base and safe_base in suspicious_base:
        # ç§»é™¤é€£å­—ç¬¦å¾Œæª¢æŸ¥æ˜¯å¦åŒ…å«åŸå§‹ç¶²åŸŸ
        base_without_dash = suspicious_base.replace('-', '')
        if safe_base in base_without_dash:
            return True
        
        # æª¢æŸ¥é€£å­—ç¬¦å‰çš„éƒ¨åˆ†æ˜¯å¦èˆ‡å®‰å…¨ç¶²åŸŸåŒ¹é…
        dash_parts = suspicious_base.split('-')
        if dash_parts[0] == safe_base:
            return True
    
    # 2. æª¢æŸ¥æ•¸å­—æ’å…¥ (pchome -> pchome24h)
    import re
    # ç§»é™¤æ•¸å­—å¾Œæª¢æŸ¥
    base_without_numbers = re.sub(r'\d+', '', suspicious_base)
    if base_without_numbers == safe_base:
        return True
    
    # 3. æª¢æŸ¥å¸¸è¦‹å¾Œç¶´æ’å…¥
    common_suffixes = ['search', 'official', 'secure', 'login', 'bank', 'pay', 'tw', 'taiwan', '24h', 'shop', 'store']
    for suffix in common_suffixes:
        if suspicious_base == safe_base + suffix:
            return True
        if suspicious_base == safe_base + '-' + suffix:
            return True
    
    # 3.5. æª¢æŸ¥å­—æ¯æ’å…¥ (apple -> apples, google -> googles)
    # æª¢æŸ¥æ˜¯å¦åœ¨åŸç¶²åŸŸå¾ŒåŠ äº†å–®å€‹æˆ–å°‘æ•¸å­—æ¯
    if suspicious_base.startswith(safe_base) and len(suspicious_base) > len(safe_base):
        added_part = suspicious_base[len(safe_base):]
        # å¸¸è¦‹çš„å­—æ¯æ·»åŠ ï¼ˆè¤‡æ•¸å½¢å¼ã€å¸¸è¦‹å¾Œç¶´ç­‰ï¼‰
        if len(added_part) <= 3 and added_part.isalpha():
            return True
    
    # 4. æª¢æŸ¥å­ç¶²åŸŸä¸­çš„è®Šå½¢æ”»æ“Šï¼ˆæ–°å¢ï¼‰
    # ä¾‹å¦‚ event.liontravel-tw.com ä¸­çš„ liontravel-tw æ˜¯å° liontravel çš„è®Šå½¢
    if len(suspicious_parts) >= 2:
        for i, suspicious_part in enumerate(suspicious_parts):
            # æª¢æŸ¥æ¯å€‹éƒ¨åˆ†æ˜¯å¦åŒ…å«å°å®‰å…¨ç¶²åŸŸçš„è®Šå½¢
            if '-' in suspicious_part and safe_base in suspicious_part:
                # æª¢æŸ¥é€£å­—ç¬¦å‰çš„éƒ¨åˆ†æ˜¯å¦èˆ‡å®‰å…¨ç¶²åŸŸåŒ¹é…
                dash_parts = suspicious_part.split('-')
                if dash_parts[0] == safe_base:
                    return True
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå®‰å…¨ç¶²åŸŸåŠ ä¸Šå¸¸è¦‹å¾Œç¶´
            for suffix in common_suffixes:
                if suspicious_part == safe_base + '-' + suffix:
                    return True
                if suspicious_part == safe_base + suffix:
                    return True
    
    # 5. åŸæœ‰çš„æ¨¡å¼æª¢æ¸¬
    insertion_patterns = [
        f"{safe_domain.split('.')[0]}-tw.{'.'.join(safe_domain.split('.')[1:])}",
        f"{safe_domain.split('.')[0]}-taiwan.{'.'.join(safe_domain.split('.')[1:])}",
        f"{safe_domain}.tw",
        f"tw.{safe_domain}",
        f"taiwan.{safe_domain}",
    ]
    
    return suspicious_domain in insertion_patterns

def _is_domain_suffix_spoofing(suspicious_domain, safe_domain):
    """æª¢æ¸¬ç¶²åŸŸå¾Œç¶´è®Šå½¢"""
    # æª¢æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„å®‰å…¨ç¶²åŸŸä½œç‚ºå‰ç¶´
    if suspicious_domain.startswith(safe_domain + '.'):
        # ä¾‹å¦‚ cht.com.tw.fake.com åŒ…å« cht.com.tw.
        return True
    
    safe_base = safe_domain.split('.')[0]  # ä¾‹å¦‚å¾ google.com å–å¾— google
    suspicious_base = suspicious_domain.split('.')[0]  # ä¾‹å¦‚å¾ google-tw.com å–å¾— google-tw
    
    # æª¢æŸ¥æ˜¯å¦åœ¨åŸºç¤ç¶²åŸŸåç¨±å¾ŒåŠ äº†é¡å¤–å­—å…ƒ
    if suspicious_base.startswith(safe_base) and len(suspicious_base) > len(safe_base):
        added_part = suspicious_base[len(safe_base):]
        # å¸¸è¦‹çš„æ·»åŠ æ¨¡å¼
        common_additions = ['-tw', '-taiwan', '-official', '-secure', '-login', '-bank', '-pay']
        return any(added_part.startswith(addition) for addition in common_additions)
    
    return False

def _is_homograph_attack(suspicious_domain, safe_domain):
    """æª¢æ¸¬åŒéŸ³å­—æˆ–ç›¸ä¼¼å­—å…ƒæ”»æ“Š - æ”¹é€²ç‰ˆ"""
    # æ“´å±•çš„å­—å…ƒæ›¿æ›å°æ‡‰è¡¨
    homograph_map = {
        'o': ['0', 'Î¿', 'Ğ¾', 'Ã¸', 'Ã¶', 'Ã²', 'Ã³', 'Ã´', 'Ãµ'],  # è‹±æ–‡oåŠå„ç¨®è®Šå½¢
        'a': ['Ğ°', 'Î±', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', '@'],  # è‹±æ–‡aåŠå„ç¨®è®Šå½¢
        'e': ['Ğµ', 'Îµ', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', '3'],  # è‹±æ–‡eåŠå„ç¨®è®Šå½¢
        'i': ['1', 'l', 'Ñ–', 'Î¹', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', '!', '|'],  # è‹±æ–‡iåŠå„ç¨®è®Šå½¢
        'l': ['1', 'i', 'Î¹', '|', 'Å‚'],  # è‹±æ–‡låŠå„ç¨®è®Šå½¢
        'n': ['Î·', 'Ã±'],  # è‹±æ–‡nåŠå„ç¨®è®Šå½¢
        'p': ['Ï'],  # è‹±æ–‡påŠå„ç¨®è®Šå½¢
        'c': ['Ñ', 'Ã§', 'Â©'],  # è‹±æ–‡cåŠå„ç¨®è®Šå½¢
        'x': ['Ñ…', 'Ã—'],  # è‹±æ–‡xåŠå„ç¨®è®Šå½¢
        'u': ['Ï…', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼'],  # è‹±æ–‡uåŠå„ç¨®è®Šå½¢
        's': ['$', '5', 'ÅŸ'],  # è‹±æ–‡såŠå„ç¨®è®Šå½¢
        'g': ['9', 'ÄŸ'],  # è‹±æ–‡gåŠå„ç¨®è®Šå½¢
        't': ['7', 'Å£'],  # è‹±æ–‡tåŠå„ç¨®è®Šå½¢
        'b': ['6', 'Î²'],  # è‹±æ–‡båŠå„ç¨®è®Šå½¢
        'd': ['Ã°'],  # è‹±æ–‡dåŠå„ç¨®è®Šå½¢
        'f': ['Æ’'],  # è‹±æ–‡fåŠå„ç¨®è®Šå½¢
        'h': ['Ä§'],  # è‹±æ–‡håŠå„ç¨®è®Šå½¢
        'k': ['Îº'],  # è‹±æ–‡kåŠå„ç¨®è®Šå½¢
        'm': ['Î¼'],  # è‹±æ–‡måŠå„ç¨®è®Šå½¢
        'r': ['Ï', 'Å™'],  # è‹±æ–‡råŠå„ç¨®è®Šå½¢
        'v': ['Î½'],  # è‹±æ–‡våŠå„ç¨®è®Šå½¢
        'w': ['Ï‰'],  # è‹±æ–‡wåŠå„ç¨®è®Šå½¢
        'y': ['Ã½', 'Ã¿'],  # è‹±æ–‡yåŠå„ç¨®è®Šå½¢
        'z': ['Î¶', 'Å¾'],  # è‹±æ–‡zåŠå„ç¨®è®Šå½¢
    }
    
    # æª¢æŸ¥æ¯å€‹å­—å…ƒæ˜¯å¦è¢«æ›¿æ›
    if len(suspicious_domain) != len(safe_domain):
        return False
    
    substitution_count = 0
    for i, (sus_char, safe_char) in enumerate(zip(suspicious_domain, safe_domain)):
        if sus_char != safe_char:
            # æª¢æŸ¥æ˜¯å¦ç‚ºå·²çŸ¥çš„ç›¸ä¼¼å­—å…ƒæ›¿æ›
            if safe_char.lower() in homograph_map:
                if sus_char in homograph_map[safe_char.lower()]:
                    substitution_count += 1
                else:
                    return False  # ä¸æ˜¯å·²çŸ¥çš„ç›¸ä¼¼å­—å…ƒæ›¿æ›
            else:
                return False  # å­—å…ƒä¸åœ¨æ›¿æ›è¡¨ä¸­
    
    # å¦‚æœæœ‰1-3å€‹å­—å…ƒè¢«æ›¿æ›ï¼Œèªç‚ºæ˜¯ç›¸ä¼¼å­—å…ƒæ”»æ“Š
    return 1 <= substitution_count <= 3 